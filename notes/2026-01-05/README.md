What did I study today?
I installed Ollama and ran open-source LLMs locally, testing different models and sizes from the command line.

What idea am I taking away?
Running models locally makes the differences between small and large LLMs very concrete and builds intuition fast.

What would I like to try tomorrow?
I want to test the same prompt across two models and compare quality, speed, and behavior.